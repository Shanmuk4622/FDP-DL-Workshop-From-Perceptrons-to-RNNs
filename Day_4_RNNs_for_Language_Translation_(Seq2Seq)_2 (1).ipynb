{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBi3BBUyl7Ue"
      },
      "source": [
        "\n",
        "### **Text Cell**\n",
        "\n",
        "# ğŸ§  FDP Day 4 (Extended): Spanish Translation\n",
        "\n",
        "Welcome\\! In the last notebook, we built a model to translate English to French.\n",
        "\n",
        "Now, we'll prove how flexible this model is by training it on a **new language (Spanish)** without changing the model's structure at all. This demonstrates a core concept in Deep Learning: the same architecture can learn different tasks *if* it's given different data.\n",
        "\n",
        "## ğŸ“ Our Plan\n",
        "\n",
        "1.  **Change Data Source:** We'll download the English-Spanish dataset instead of the English-French one.\n",
        "2.  **Inspect Data:** We'll add new steps to look at our raw data and understand our new \"vocabulary.\"\n",
        "3.  **Tokenize:** We'll create our character-to-index maps for English and **Spanish**.\n",
        "4.  **Vectorize:** We'll one-hot encode our data (this code is identical).\n",
        "5.  **Build Model:** We'll use the *exact same* Encoder-Decoder model as before.\n",
        "6.  **Train Model:** We'll train the model on the Spanish sentences.\n",
        "7.  **Run Inference:** We'll create the inference models (this code is identical).\n",
        "8.  **Test:** We'll write a **new, simple function** that lets you type any English sentence and get a Spanish translation\\!\n",
        "\n",
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 1: Setup - Importing Libraries\n",
        "\n",
        "No changes here. We still need TensorFlow, Keras, and Numpy.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKfI4TSZl7Uf",
        "outputId": "10ea47e3-7544-421f-8420-17ea295a5a45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, GRU, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"Libraries imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gZqL91El7Ug"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 2: Define Dataset Parameters\n",
        "\n",
        "These parameters are good starting points, so we'll keep them. We'll train on the first `10,000` samples. For a real-world model, you would want *much* more data (the full dataset has over 140,000 sentences\\!) and train for more epochs.\n",
        "\n",
        "*Note:* Training for 100 epochs will take some time. For a quick test, you can set `EPOCHS = 20`.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xQ_lI9iyl7Uh"
      },
      "outputs": [],
      "source": [
        "# --- Parameters ---\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "LATENT_DIM = 256  # This is the \"size\" of the thought vector\n",
        "NUM_SAMPLES = 10000 # Number of sentences to train on\n",
        "\n",
        "# --- Data Path ---\n",
        "# This will be our new data file!\n",
        "DATA_PATH = \"spa.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyK887-Vl7Uh"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 3: Download and Unzip the **Spanish** Data\n",
        "\n",
        "This is our first major change\\! Instead of `fra-eng.zip`, we are downloading `spa-eng.zip`. The `manythings.org` website provides many language pairs in this exact same format, making it easy to swap.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YivrvVHcl7Uh",
        "outputId": "bf37a777-cf29-4f36-dfbc-50bc4d3c6f92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 5338k  100 5338k    0     0  9245k      0 --:--:-- --:--:-- --:--:-- 9251k\n",
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n",
            "Spanish data downloaded and unzipped.\n"
          ]
        }
      ],
      "source": [
        "# Download the data\n",
        "!curl -O http://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip -o spa-eng.zip\n",
        "\n",
        "print(\"Spanish data downloaded and unzipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu_Hzwl3l7Uh"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 4: Parse the Data\n",
        "\n",
        "The parsing logic is identical. We read the file line by line. Each line is:\n",
        "`English Sentence` \\<tab\\> `Spanish Sentence` \\<tab\\> `Attribution`\n",
        "\n",
        "We'll store the English in `input_texts` and the Spanish in `target_texts`.\n",
        "\n",
        "We still add `\\t` (tab) as the \"start\" token and `\\n` (newline) as the \"end\" token to our target (Spanish) sentences. This is crucial for the decoder to learn *when to start* and *when to stop* generating text.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBzvnj35l7Ui",
        "outputId": "f60cf698-1391-424b-8d28-63e8d563e843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10000 sentence pairs.\n"
          ]
        }
      ],
      "source": [
        "# --- Parse the data --\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "# Read the file\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "\n",
        "# Process each line\n",
        "for line in lines[: min(NUM_SAMPLES, len(lines) - 1)]:\n",
        "    # Split the line into English (input) and Spanish (target)\n",
        "    try:\n",
        "        input_text, target_text, _ = line.split(\"\\t\")\n",
        "    except:\n",
        "        print(f\"Skipping bad line: {line}\")\n",
        "        continue\n",
        "\n",
        "    # We use \"tab\" as the \"start sequence\" token\n",
        "    # and \"\\n\" as the \"end sequence\" token for the target.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "\n",
        "    # Add any new characters to our \"vocabulary\" sets\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "print(f\"Loaded {len(input_texts)} sentence pairs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHhsd6aNl7Ui"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 5: **New Cell** - Inspect the Data\n",
        "\n",
        "Let's add a step to see what we've loaded. This is always a good idea. We'll print the first 10 sentence pairs. Notice the `\\t` and `\\n` in the Spanish sentences.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEcn1hh8l7Uj",
        "outputId": "469327a0-a726-4538-912e-c33d3048ecad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 10 Sample Pairs ---\n",
            "Input:    Go.\n",
            "Target:   '\\tVe.\\n'\n",
            "---\n",
            "Input:    Go.\n",
            "Target:   '\\tVete.\\n'\n",
            "---\n",
            "Input:    Go.\n",
            "Target:   '\\tVaya.\\n'\n",
            "---\n",
            "Input:    Go.\n",
            "Target:   '\\tVÃ¡yase.\\n'\n",
            "---\n",
            "Input:    Hi.\n",
            "Target:   '\\tHola.\\n'\n",
            "---\n",
            "Input:    Run!\n",
            "Target:   '\\tÂ¡Corre!\\n'\n",
            "---\n",
            "Input:    Run!\n",
            "Target:   '\\tÂ¡Corran!\\n'\n",
            "---\n",
            "Input:    Run!\n",
            "Target:   '\\tÂ¡Huye!\\n'\n",
            "---\n",
            "Input:    Run!\n",
            "Target:   '\\tÂ¡Corra!\\n'\n",
            "---\n",
            "Input:    Run!\n",
            "Target:   '\\tÂ¡Corred!\\n'\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "print(\"--- 10 Sample Pairs ---\")\n",
        "for i in range(10):\n",
        "    print(f\"Input:    {input_texts[i]}\")\n",
        "    # We use repr() to show the \"hidden\" \\t and \\n characters\n",
        "    print(f\"Target:   {repr(target_texts[i])}\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrytTmOTl7Uj"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 6: Create the Character Vocabularies\n",
        "\n",
        "Now we find all the unique characters in our dataset. This is our \"vocabulary.\"\n",
        "\n",
        "Why use **character-level** tokenization?\n",
        "\n",
        "  * **Small Vocabulary:** Our model only needs to learn \\~70-100 tokens (characters). A *word-level* model would need to learn 50,000+ unique words, which is much bigger.\n",
        "  * **Handles Typos & New Words:** A character model can spell out any word, even ones it hasn't seen (like \"Googol\"). A word model would see this as an \"Unknown\" token.\n",
        "  * **The Downside:** It's slower and has to learn grammar and spelling from scratch.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TavPE9-l7Uj",
        "outputId": "064ab1a8-7b12-4750-ff68-2fe53ca7c262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Stats ---\n",
            "Number of unique input tokens (English characters): 68\n",
            "Number of unique output tokens (Spanish characters): 84\n",
            "Max sequence length for inputs: 16\n",
            "Max sequence length for outputs: 47\n"
          ]
        }
      ],
      "source": [
        "# Sort the vocabularies\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "# Get the size of our vocabularies and longest sentences\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"\\n--- Data Stats ---\")\n",
        "print(\"Number of unique input tokens (English characters):\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens (Spanish characters):\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OncT2jC6l7Uj"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 7: Create Character-to-Index Maps\n",
        "\n",
        "Models don't read letters. They read numbers. This step creates a simple dictionary (a \"lookup map\") that converts each character to a unique number.\n",
        "\n",
        "`'a'` -\\> `10`\n",
        "`'b'` -\\> `11`\n",
        "etc.\n",
        "\n",
        "We'll print the first 15 mappings for both languages. Notice that the Spanish vocabulary (`target_token_index`) includes special characters like `Â¿`, `Â¡`, and `Ã±` that the English one doesn't.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwgP0Epcl7Uj",
        "outputId": "4a996fd1-70d5-48ef-c604-19d3309fb240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Input Token Index (English) ---\n",
            "[(' ', 0), ('!', 1), ('$', 2), ('&', 3), (\"'\", 4), (',', 5), ('.', 6), ('0', 7), ('1', 8), ('2', 9), ('3', 10), ('5', 11), ('6', 12), ('7', 13), ('8', 14)]\n",
            "\n",
            "--- Target Token Index (Spanish) ---\n",
            "[('\\t', 0), ('\\n', 1), (' ', 2), ('!', 3), ('\"', 4), ('&', 5), (\"'\", 6), (',', 7), ('-', 8), ('.', 9), ('0', 10), ('1', 11), ('2', 12), ('3', 13), ('5', 14)]\n"
          ]
        }
      ],
      "source": [
        "# Create the character-to-index lookup maps\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "print(\"--- Input Token Index (English) ---\")\n",
        "print(list(input_token_index.items())[:15])\n",
        "\n",
        "print(\"\\n--- Target Token Index (Spanish) ---\")\n",
        "print(list(target_token_index.items())[:15])\n",
        "# Note: You should see special Spanish characters in this list!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPKOAaIVl7Uj"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 8: Vectorization (One-Hot Encoding)\n",
        "\n",
        "This is the most important pre-processing step. We create three massive 3D arrays to hold our data.\n",
        "\n",
        "A 3D array (or \"Tensor\") has a shape:\n",
        "`(samples, timesteps, features)`\n",
        "\n",
        "  * **samples:** The number of sentences (10,000).\n",
        "  * **timesteps:** The maximum length of a sentence (e.g., `max_encoder_seq_length`).\n",
        "  * **features:** The size of our vocabulary (e.g., `num_encoder_tokens`).\n",
        "\n",
        "We are using **One-Hot Encoding**. This means for each character, we create a vector that is all `0`s *except* for a single `1` at the index of that character.\n",
        "\n",
        "If `'H'` is index `35`, the vector for `'H'` would be:\n",
        "`[0, 0, 0, ... 0, 1, 0, ... 0]` (with the `1` at position 35).\n",
        "\n",
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 9: Understanding \"Teacher Forcing\"\n",
        "\n",
        "You'll see we create *two* target arrays: `decoder_input_data` and `decoder_target_data`. This is for a technique called **Teacher Forcing**.\n",
        "\n",
        "The model needs to learn: \"Given the *previous* character, predict the *next* one.\"\n",
        "\n",
        "  * **`decoder_input_data`** is the \"question.\" It's the Spanish sentence starting with `\\t`.\n",
        "  * **`decoder_target_data`** is the \"answer.\" It's the *same sentence* shifted one step forward.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "  * **Input Sentence:** `Hola`\n",
        "  * `decoder_input_data` will be: `\\t`, `H`, `o`, `l`, `a`\n",
        "  * `decoder_target_data` will be: `H`, `o`, `l`, `a`, `\\n`\n",
        "\n",
        "The model learns:\n",
        "\n",
        "1.  When you see `\\t`, predict `H`.\n",
        "2.  When you see `H`, predict `o`.\n",
        "3.  When you see `o`, predict `l`.\n",
        "    ...and so on.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvrCk37tl7Uj",
        "outputId": "7358366f-41f1-4561-ee22-8e0b89d4762a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created empty data arrays...\n",
            "Data vectorization (one-hot encoding) complete.\n",
            "Shape of encoder_input_data: (10000, 16, 68)\n",
            "Shape of decoder_input_data: (10000, 47, 84)\n",
            "Shape of decoder_target_data: (10000, 47, 84)\n"
          ]
        }
      ],
      "source": [
        "# --- Create the empty numpy arrays ---\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "print(\"Created empty data arrays...\")\n",
        "\n",
        "# --- Fill the arrays with one-hot vectors ---\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    # Fill encoder_input_data\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    # Pad the rest of the sequence with spaces\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "\n",
        "    # Fill decoder_input_data and decoder_target_data\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_input_data is the sequence itself\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data is the sequence shifted by one\n",
        "            # (it does not include the start token '\\t')\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    # Pad the rest of the sequences with spaces\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "print(\"Data vectorization (one-hot encoding) complete.\")\n",
        "print(f\"Shape of encoder_input_data: {encoder_input_data.shape}\")\n",
        "print(f\"Shape of decoder_input_data: {decoder_input_data.shape}\")\n",
        "print(f\"Shape of decoder_target_data: {decoder_target_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTU8pdefl7Uk"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 10: Define the Encoder\n",
        "\n",
        "This part is **identical** to the French notebook. Its job is to read the *entire* English sentence, one character at a time, and compress its meaning into a single vector of size `LATENT_DIM` (256).\n",
        "\n",
        "This \"thought vector\" (or \"context vector\") is `state_h`. It's the only thing we pass to the decoder.\n",
        "\n",
        "(We include `reset_after=False` to fix a known TensorFlow bug with GRU/LSTM layers).\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z2E4Hm7l7Uk",
        "outputId": "11faffad-ea74-4088-eb23-2aeea0be4510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder built.\n"
          ]
        }
      ],
      "source": [
        "# --- Encoder ---\n",
        "# Define the input layer\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "# Define the GRU layer\n",
        "# We set return_state=True to get the final \"thought vector\"\n",
        "encoder_gru = GRU(LATENT_DIM, return_state=True, reset_after=False)\n",
        "\n",
        "# Run the inputs through the GRU\n",
        "encoder_outputs, state_h = encoder_gru(encoder_inputs)\n",
        "\n",
        "# We discard the `encoder_outputs` and only keep the state (`state_h`)\n",
        "encoder_states = [state_h]\n",
        "\n",
        "print(\"Encoder built.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbRYY_fGl7Uk"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 11: Define the Decoder\n",
        "\n",
        "This is also **identical**. Its job is to generate the Spanish sentence.\n",
        "\n",
        "It's more complex than the encoder:\n",
        "\n",
        "1.  It takes the `decoder_inputs` (the \"teacher-forced\" Spanish sentence).\n",
        "2.  It takes the `encoder_states` (the \"thought vector\" from the encoder) as its **`initial_state`**. This is the magic link\\! This is how the decoder knows *what* to translate.\n",
        "3.  It `return_sequences=True` because it needs to output a prediction at *every* time step (for every character).\n",
        "4.  The final `Dense` layer with `softmax` is a classifier. It looks at the GRU's output and predicts \"which character in the Spanish vocabulary is most likely to be next?\"\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIAKMOYVl7Uk",
        "outputId": "42baa8ed-4103-43db-86e1-7ddb6fc22550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder built.\n"
          ]
        }
      ],
      "source": [
        "# --- Decoder ---\n",
        "# Define the input layer\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# Define the GRU layer\n",
        "decoder_gru = GRU(LATENT_DIM, return_sequences=True, return_state=True, reset_after=False)\n",
        "\n",
        "# Run the inputs through the GRU, using the ENCODER'S state as the STARTING state\n",
        "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "# A Dense layer to act as our character-classifier\n",
        "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "\n",
        "# Apply the classifier to the GRU's outputs\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "print(\"Decoder built.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AUpxU_Vl7Uk"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 12: Define and Compile the **Training** Model\n",
        "\n",
        "This is the final model we will *train*. It connects the inputs and outputs we just defined.\n",
        "\n",
        "  * **Inputs:** `encoder_inputs` (English) and `decoder_inputs` (Spanish, for teacher-forcing).\n",
        "  * **Outputs:** `decoder_outputs` (the model's predictions).\n",
        "\n",
        "We compile it with `categorical_crossentropy` because this is a classification problem (choosing one character from many).\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "fYIdrEZnl7Uk",
        "outputId": "191d2dd3-70d8-4827-d1bf-83e2fcf48772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model compiled.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ input_layer_1       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)           â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">249,600</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
              "â”‚                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">261,888</span> â”‚ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
              "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚            â”‚ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]         â”‚\n",
              "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">21,588</span> â”‚ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ input_layer_1       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru (\u001b[38;5;33mGRU\u001b[0m)           â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     â”‚    \u001b[38;5;34m249,600\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
              "â”‚                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     â”‚    \u001b[38;5;34m261,888\u001b[0m â”‚ input_layer_1[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
              "â”‚                     â”‚ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      â”‚            â”‚ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]         â”‚\n",
              "â”‚                     â”‚ \u001b[38;5;34m256\u001b[0m)]             â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)  â”‚     \u001b[38;5;34m21,588\u001b[0m â”‚ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">533,076</span> (2.03 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m533,076\u001b[0m (2.03 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">533,076</span> (2.03 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m533,076\u001b[0m (2.03 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Full Model ---\n",
        "# This is the model we will *train*\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "print(\"Model compiled.\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzfIqT-dl7Uk"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 13: Train the Model\n",
        "\n",
        "This is the moment of truth. We `fit` the model, passing our two inputs and one target.\n",
        "\n",
        "This will take a few minutes. If you are using a GPU in Colab (go to `Runtime > Change runtime type > T4 GPU`), it will be much faster.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij2YLdAol7Uk",
        "outputId": "e4addf02-8dd7-4c81-ba0f-6e1adcd26212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting model training...\n",
            "Epoch 1/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - accuracy: 0.6701 - loss: 1.7492 - val_accuracy: 0.6579 - val_loss: 1.2217\n",
            "Epoch 2/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7252 - loss: 1.0219 - val_accuracy: 0.7168 - val_loss: 0.9921\n",
            "Epoch 3/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7605 - loss: 0.8532 - val_accuracy: 0.7365 - val_loss: 0.8984\n",
            "Epoch 4/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7812 - loss: 0.7500 - val_accuracy: 0.7528 - val_loss: 0.8267\n",
            "Epoch 5/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7909 - loss: 0.7098 - val_accuracy: 0.7586 - val_loss: 0.8009\n",
            "Epoch 6/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7931 - loss: 0.6889 - val_accuracy: 0.7677 - val_loss: 0.7799\n",
            "Epoch 7/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7986 - loss: 0.6691 - val_accuracy: 0.7697 - val_loss: 0.7615\n",
            "Epoch 8/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8016 - loss: 0.6565 - val_accuracy: 0.7709 - val_loss: 0.7554\n",
            "Epoch 9/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8050 - loss: 0.6429 - val_accuracy: 0.7752 - val_loss: 0.7478\n",
            "Epoch 10/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8081 - loss: 0.6352 - val_accuracy: 0.7792 - val_loss: 0.7323\n",
            "Epoch 11/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8114 - loss: 0.6250 - val_accuracy: 0.7826 - val_loss: 0.7179\n",
            "Epoch 12/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8150 - loss: 0.6131 - val_accuracy: 0.7830 - val_loss: 0.7126\n",
            "Epoch 13/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8169 - loss: 0.6042 - val_accuracy: 0.7872 - val_loss: 0.6990\n",
            "Epoch 14/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8199 - loss: 0.5951 - val_accuracy: 0.7914 - val_loss: 0.6892\n",
            "Epoch 15/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8237 - loss: 0.5822 - val_accuracy: 0.7942 - val_loss: 0.6812\n",
            "Epoch 16/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8275 - loss: 0.5676 - val_accuracy: 0.7973 - val_loss: 0.6637\n",
            "Epoch 17/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8298 - loss: 0.5612 - val_accuracy: 0.8017 - val_loss: 0.6546\n",
            "Epoch 18/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8341 - loss: 0.5464 - val_accuracy: 0.8044 - val_loss: 0.6460\n",
            "Epoch 19/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8351 - loss: 0.5410 - val_accuracy: 0.8055 - val_loss: 0.6405\n",
            "Epoch 20/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8395 - loss: 0.5290 - val_accuracy: 0.8065 - val_loss: 0.6374\n",
            "Epoch 21/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8412 - loss: 0.5213 - val_accuracy: 0.8125 - val_loss: 0.6202\n",
            "Epoch 22/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8432 - loss: 0.5150 - val_accuracy: 0.8132 - val_loss: 0.6168\n",
            "Epoch 23/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8483 - loss: 0.4976 - val_accuracy: 0.8127 - val_loss: 0.6148\n",
            "Epoch 24/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8481 - loss: 0.4986 - val_accuracy: 0.8165 - val_loss: 0.6069\n",
            "Epoch 25/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8511 - loss: 0.4881 - val_accuracy: 0.8186 - val_loss: 0.6044\n",
            "Epoch 26/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8539 - loss: 0.4783 - val_accuracy: 0.8179 - val_loss: 0.5979\n",
            "Epoch 27/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8554 - loss: 0.4735 - val_accuracy: 0.8210 - val_loss: 0.5925\n",
            "Epoch 28/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8568 - loss: 0.4693 - val_accuracy: 0.8249 - val_loss: 0.5827\n",
            "Epoch 29/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8593 - loss: 0.4620 - val_accuracy: 0.8246 - val_loss: 0.5828\n",
            "Epoch 30/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8611 - loss: 0.4560 - val_accuracy: 0.8256 - val_loss: 0.5835\n",
            "Epoch 31/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8633 - loss: 0.4464 - val_accuracy: 0.8275 - val_loss: 0.5749\n",
            "Epoch 32/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8659 - loss: 0.4407 - val_accuracy: 0.8286 - val_loss: 0.5728\n",
            "Epoch 33/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8685 - loss: 0.4319 - val_accuracy: 0.8303 - val_loss: 0.5679\n",
            "Epoch 34/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8710 - loss: 0.4250 - val_accuracy: 0.8306 - val_loss: 0.5653\n",
            "Epoch 35/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8703 - loss: 0.4260 - val_accuracy: 0.8317 - val_loss: 0.5665\n",
            "Epoch 36/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.8740 - loss: 0.4158 - val_accuracy: 0.8340 - val_loss: 0.5630\n",
            "Epoch 37/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8751 - loss: 0.4116 - val_accuracy: 0.8342 - val_loss: 0.5587\n",
            "Epoch 38/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8781 - loss: 0.4025 - val_accuracy: 0.8351 - val_loss: 0.5584\n",
            "Epoch 39/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8794 - loss: 0.3980 - val_accuracy: 0.8370 - val_loss: 0.5544\n",
            "Epoch 40/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8827 - loss: 0.3888 - val_accuracy: 0.8371 - val_loss: 0.5521\n",
            "Epoch 41/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8839 - loss: 0.3854 - val_accuracy: 0.8382 - val_loss: 0.5549\n",
            "Epoch 42/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8852 - loss: 0.3793 - val_accuracy: 0.8395 - val_loss: 0.5493\n",
            "Epoch 43/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8866 - loss: 0.3739 - val_accuracy: 0.8394 - val_loss: 0.5504\n",
            "Epoch 44/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8897 - loss: 0.3663 - val_accuracy: 0.8426 - val_loss: 0.5432\n",
            "Epoch 45/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8907 - loss: 0.3601 - val_accuracy: 0.8395 - val_loss: 0.5497\n",
            "Epoch 46/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8918 - loss: 0.3589 - val_accuracy: 0.8422 - val_loss: 0.5447\n",
            "Epoch 47/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8929 - loss: 0.3522 - val_accuracy: 0.8436 - val_loss: 0.5439\n",
            "Epoch 48/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8954 - loss: 0.3468 - val_accuracy: 0.8431 - val_loss: 0.5448\n",
            "Epoch 49/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8974 - loss: 0.3391 - val_accuracy: 0.8446 - val_loss: 0.5407\n",
            "Epoch 50/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8996 - loss: 0.3342 - val_accuracy: 0.8455 - val_loss: 0.5424\n",
            "Epoch 51/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9011 - loss: 0.3270 - val_accuracy: 0.8446 - val_loss: 0.5449\n",
            "Epoch 52/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9033 - loss: 0.3215 - val_accuracy: 0.8454 - val_loss: 0.5456\n",
            "Epoch 53/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9039 - loss: 0.3172 - val_accuracy: 0.8449 - val_loss: 0.5448\n",
            "Epoch 54/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9049 - loss: 0.3143 - val_accuracy: 0.8454 - val_loss: 0.5455\n",
            "Epoch 55/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9076 - loss: 0.3059 - val_accuracy: 0.8458 - val_loss: 0.5499\n",
            "Epoch 56/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9081 - loss: 0.3024 - val_accuracy: 0.8466 - val_loss: 0.5510\n",
            "Epoch 57/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9105 - loss: 0.2961 - val_accuracy: 0.8456 - val_loss: 0.5506\n",
            "Epoch 58/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9108 - loss: 0.2944 - val_accuracy: 0.8452 - val_loss: 0.5544\n",
            "Epoch 59/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9117 - loss: 0.2916 - val_accuracy: 0.8461 - val_loss: 0.5566\n",
            "Epoch 60/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9157 - loss: 0.2815 - val_accuracy: 0.8468 - val_loss: 0.5578\n",
            "Epoch 61/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9152 - loss: 0.2793 - val_accuracy: 0.8473 - val_loss: 0.5605\n",
            "Epoch 62/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9167 - loss: 0.2744 - val_accuracy: 0.8444 - val_loss: 0.5705\n",
            "Epoch 63/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9176 - loss: 0.2718 - val_accuracy: 0.8461 - val_loss: 0.5659\n",
            "Epoch 64/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9191 - loss: 0.2680 - val_accuracy: 0.8466 - val_loss: 0.5708\n",
            "Epoch 65/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9205 - loss: 0.2634 - val_accuracy: 0.8475 - val_loss: 0.5722\n",
            "Epoch 66/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9219 - loss: 0.2578 - val_accuracy: 0.8474 - val_loss: 0.5760\n",
            "Epoch 67/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9232 - loss: 0.2547 - val_accuracy: 0.8442 - val_loss: 0.5855\n",
            "Epoch 68/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9241 - loss: 0.2508 - val_accuracy: 0.8464 - val_loss: 0.5792\n",
            "Epoch 69/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9255 - loss: 0.2465 - val_accuracy: 0.8469 - val_loss: 0.5824\n",
            "Epoch 70/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9273 - loss: 0.2417 - val_accuracy: 0.8473 - val_loss: 0.5858\n",
            "Epoch 71/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9280 - loss: 0.2382 - val_accuracy: 0.8467 - val_loss: 0.5940\n",
            "Epoch 72/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9286 - loss: 0.2361 - val_accuracy: 0.8474 - val_loss: 0.5945\n",
            "Epoch 73/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9296 - loss: 0.2330 - val_accuracy: 0.8474 - val_loss: 0.6034\n",
            "Epoch 74/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9315 - loss: 0.2281 - val_accuracy: 0.8452 - val_loss: 0.6044\n",
            "Epoch 75/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9320 - loss: 0.2256 - val_accuracy: 0.8465 - val_loss: 0.6068\n",
            "Epoch 76/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9336 - loss: 0.2214 - val_accuracy: 0.8445 - val_loss: 0.6146\n",
            "Epoch 77/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9349 - loss: 0.2160 - val_accuracy: 0.8459 - val_loss: 0.6205\n",
            "Epoch 78/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9353 - loss: 0.2154 - val_accuracy: 0.8436 - val_loss: 0.6323\n",
            "Epoch 79/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9362 - loss: 0.2126 - val_accuracy: 0.8456 - val_loss: 0.6297\n",
            "Epoch 80/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9370 - loss: 0.2092 - val_accuracy: 0.8449 - val_loss: 0.6297\n",
            "Epoch 81/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9387 - loss: 0.2030 - val_accuracy: 0.8464 - val_loss: 0.6333\n",
            "Epoch 82/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9389 - loss: 0.2017 - val_accuracy: 0.8446 - val_loss: 0.6382\n",
            "Epoch 83/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9393 - loss: 0.1993 - val_accuracy: 0.8448 - val_loss: 0.6452\n",
            "Epoch 84/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9417 - loss: 0.1953 - val_accuracy: 0.8456 - val_loss: 0.6430\n",
            "Epoch 85/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9412 - loss: 0.1946 - val_accuracy: 0.8462 - val_loss: 0.6495\n",
            "Epoch 86/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9430 - loss: 0.1889 - val_accuracy: 0.8445 - val_loss: 0.6560\n",
            "Epoch 87/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9434 - loss: 0.1864 - val_accuracy: 0.8452 - val_loss: 0.6594\n",
            "Epoch 88/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9438 - loss: 0.1861 - val_accuracy: 0.8453 - val_loss: 0.6625\n",
            "Epoch 89/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9449 - loss: 0.1825 - val_accuracy: 0.8444 - val_loss: 0.6709\n",
            "Epoch 90/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9456 - loss: 0.1792 - val_accuracy: 0.8454 - val_loss: 0.6766\n",
            "Epoch 91/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9474 - loss: 0.1747 - val_accuracy: 0.8434 - val_loss: 0.6809\n",
            "Epoch 92/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9475 - loss: 0.1736 - val_accuracy: 0.8440 - val_loss: 0.6851\n",
            "Epoch 93/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9485 - loss: 0.1720 - val_accuracy: 0.8425 - val_loss: 0.6899\n",
            "Epoch 94/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9488 - loss: 0.1703 - val_accuracy: 0.8439 - val_loss: 0.6956\n",
            "Epoch 95/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9498 - loss: 0.1665 - val_accuracy: 0.8433 - val_loss: 0.7023\n",
            "Epoch 96/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9502 - loss: 0.1634 - val_accuracy: 0.8440 - val_loss: 0.7040\n",
            "Epoch 97/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9512 - loss: 0.1619 - val_accuracy: 0.8440 - val_loss: 0.7062\n",
            "Epoch 98/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9516 - loss: 0.1603 - val_accuracy: 0.8433 - val_loss: 0.7154\n",
            "Epoch 99/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9522 - loss: 0.1591 - val_accuracy: 0.8425 - val_loss: 0.7190\n",
            "Epoch 100/100\n",
            "\u001b[1m125/125\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9529 - loss: 0.1558 - val_accuracy: 0.8440 - val_loss: 0.7176\n",
            "\n",
            "Training complete.\n",
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting model training...\")\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data], # Pass both inputs\n",
        "    decoder_target_data,                      # Pass the target\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "# Save the model\n",
        "model.save(\"seq2seq_spanish_translator.keras\")\n",
        "print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnRqxAejl7Uk"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 14: The Problem with Our Training Model\n",
        "\n",
        "**STOP\\!** Our training is done, but we have a problem.\n",
        "\n",
        "The model we just trained (`model`) is **unusable for real translation**.\n",
        "\n",
        "Why? Because it was built with *Teacher Forcing*. It **requires** you to feed it the correct Spanish translation in `decoder_inputs` to work. But if we already *had* the Spanish translation, we wouldn't need a model\\!\n",
        "\n",
        "We need a new setup for *inference* (translation). We will do this by re-using the *layers* we just trained, but connecting them in a new way.\n",
        "\n",
        "We need two new models:\n",
        "\n",
        "1.  **`encoder_model`**: Takes an English sentence and just gives us the \"thought vector\" (`encoder_states`).\n",
        "2.  **`decoder_model`**: Takes one character at a time (plus the previous thought vector) and predicts the *next* character and the *new* thought vector.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 15: Create the Encoder Inference Model\n",
        "\n",
        "This is simple. We define a new `Model` that takes the *original* `encoder_inputs` and outputs the *original* `encoder_states`. This model's only job is to create the \"thought vector.\"\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "pfFSMn8Vl7Uk",
        "outputId": "f10955fc-b590-4738-de93-992c130a7b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder (Inference) Model built.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)       â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">249,600</span> â”‚\n",
              "â”‚                                 â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]                  â”‚               â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m)       â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru (\u001b[38;5;33mGRU\u001b[0m)                       â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   â”‚       \u001b[38;5;34m249,600\u001b[0m â”‚\n",
              "â”‚                                 â”‚ \u001b[38;5;34m256\u001b[0m)]                  â”‚               â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">249,600</span> (975.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m249,600\u001b[0m (975.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">249,600</span> (975.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m249,600\u001b[0m (975.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# We create a new model that just runs the encoder\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "print(\"Encoder (Inference) Model built.\")\n",
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaKucErNl7Uk"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 16: Create the Decoder Inference Model\n",
        "\n",
        "This is the trickiest part. We have to re-build the decoder manually.\n",
        "\n",
        "This model will be run in a loop.\n",
        "\n",
        "1.  **Input:** It needs the *last predicted character* (as a one-hot vector) AND the *previous state vector* from the encoder (or the last loop).\n",
        "2.  **Output:** It will give us the *prediction* for the next character (a softmax probability vector) AND the *new state vector* to be used in the next loop.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "g_d-OpExl7Uk",
        "outputId": "1b2a118a-f758-4f63-9d0e-7ce889906a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder (Inference) Model built.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_3       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)     â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ input_layer_2       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),  â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">261,888</span> â”‚ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
              "â”‚                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      â”‚            â”‚ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">21,588</span> â”‚ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer_3       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m84\u001b[0m)     â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ input_layer_2       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m),  â”‚    \u001b[38;5;34m261,888\u001b[0m â”‚ input_layer_3[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
              "â”‚                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      â”‚            â”‚ input_layer_2[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m84\u001b[0m)     â”‚     \u001b[38;5;34m21,588\u001b[0m â”‚ gru_1[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">283,476</span> (1.08 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m283,476\u001b[0m (1.08 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">283,476</span> (1.08 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m283,476\u001b[0m (1.08 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Decoder (Inference) ---\n",
        "# We need to define new inputs for the state, as it will be fed in a loop\n",
        "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
        "decoder_states_inputs = [decoder_state_input_h]\n",
        "\n",
        "# We need a new input for the single character\n",
        "# Shape: (batch_size, 1, num_decoder_tokens) - we use 1 for the timestep\n",
        "decoder_inputs_single_step = Input(shape=(1, num_decoder_tokens))\n",
        "\n",
        "# We use the *original* GRU layer, but call it on our new inputs\n",
        "# We get the outputs and the new state\n",
        "decoder_outputs, state_h_dec = decoder_gru(\n",
        "    decoder_inputs_single_step, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec]\n",
        "\n",
        "# We use the *original* dense layer\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# This is our final inference-mode decoder\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_single_step] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "print(\"Decoder (Inference) Model built.\")\n",
        "decoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU50KUdRl7Ul"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 17: Create the `decode_sequence` Function\n",
        "\n",
        "This function ties everything together. It contains the \"inference loop\" logic.\n",
        "\n",
        "I have added detailed comments to explain each step.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2UCgbBBl7Ul",
        "outputId": "78c5876f-a3f0-4823-a940-8a2890425660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reverse lookup dictionaries created.\n",
            "\n",
            "Decode function created.\n"
          ]
        }
      ],
      "source": [
        "# --- Setup reverse lookup ---\n",
        "# We need to go from index-back-to-character to read the output\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "print(\"Reverse lookup dictionaries created.\")\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # 1. ENCODE: Run the input sentence through the encoder\n",
        "    # This gives us the \"thought vector\" (states_value)\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # 2. START: Create a 1-character-long \"target\" sequence\n",
        "    # This character is the \"start\" token ('\\t')\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # 3. LOOP: Start our translation loop\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    while not stop_condition:\n",
        "        # 4. PREDICT: Feed the current token and the state into the decoder\n",
        "        output_tokens, h = decoder_model.predict([target_seq] + [states_value], verbose=0)\n",
        "\n",
        "        # 5. SAMPLE: Find the most likely next character (the one with the highest prob)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "\n",
        "        # 6. APPEND: Add the new character to our sentence\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # 7. CHECK: Stop if we hit the \"end\" token ('\\n') or the max length\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # 8. UPDATE: Prepare for the next loop\n",
        "        # The new \"current token\" is the character we just predicted\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # The new \"state\" is the state `h` that the decoder just returned\n",
        "        states_value = h\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "print(\"\\nDecode function created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y_M_nm8l7Ul"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 18: Test on Training Samples\n",
        "\n",
        "Let's see how our model did. We'll run it on 20 random samples from the training set.\n",
        "\n",
        "You can see that for simple sentences, it learns quite well\\! For more complex ones, it might struggle. This is because we only used 10,000 samples.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjujBcGOl7Ul",
        "outputId": "a9d54ec4-6dd2-460f-cf11-2b2c44eb19f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Sample #0\n",
            "Input:    Go.\n",
            "Actual:   Ve.\n",
            "Predicted: Vete.\n",
            "------------------------------\n",
            "Sample #10\n",
            "Input:    Run.\n",
            "Actual:   Corra.\n",
            "Predicted: Prueba un poco.\n",
            "------------------------------\n",
            "Sample #20\n",
            "Input:    Fire!\n",
            "Actual:   Â¡Incendio!\n",
            "Predicted: Â¡De perdudo!\n",
            "------------------------------\n",
            "Sample #30\n",
            "Input:    Stay.\n",
            "Actual:   QuÃ©dese.\n",
            "Predicted: QuÃ©date.\n",
            "------------------------------\n",
            "Sample #40\n",
            "Input:    Wait.\n",
            "Actual:   Espera.\n",
            "Predicted: Esperen.\n",
            "------------------------------\n",
            "Sample #50\n",
            "Input:    Hurry!\n",
            "Actual:   Â¡Daos prisa!\n",
            "Predicted: Â¡AquÃ­ esto!\n",
            "------------------------------\n",
            "Sample #60\n",
            "Input:    I won!\n",
            "Actual:   Â¡He ganado yo!\n",
            "Predicted: Â¡He ganado!\n",
            "------------------------------\n",
            "Sample #70\n",
            "Input:    Shoot!\n",
            "Actual:   Â¡Dispare!\n",
            "Predicted: Â¡Disparen!\n",
            "------------------------------\n",
            "Sample #80\n",
            "Input:    Eat it.\n",
            "Actual:   CÃ³metelo.\n",
            "Predicted: EspÃ©jadlo.\n",
            "------------------------------\n",
            "Sample #90\n",
            "Input:    Go now.\n",
            "Actual:   Ve ahora mismo.\n",
            "Predicted: Ve ya.\n",
            "------------------------------\n",
            "Sample #100\n",
            "Input:    Got it?\n",
            "Actual:   Â¿Entendiste?\n",
            "Predicted: Â¿Esto es dementar?\n",
            "------------------------------\n",
            "Sample #110\n",
            "Input:    I fled.\n",
            "Actual:   Me escapaba.\n",
            "Predicted: Me escapaba.\n",
            "------------------------------\n",
            "Sample #120\n",
            "Input:    I sang.\n",
            "Actual:   CantÃ©.\n",
            "Predicted: Me quedÃ©.\n",
            "------------------------------\n",
            "Sample #130\n",
            "Input:    Inhale.\n",
            "Actual:   Inspiren.\n",
            "Predicted: Inspiren.\n",
            "------------------------------\n",
            "Sample #140\n",
            "Input:    No way!\n",
            "Actual:   Â¡Ni cagando!\n",
            "Predicted: Â¡Dijerlo!\n",
            "------------------------------\n",
            "Sample #150\n",
            "Input:    Try me.\n",
            "Actual:   A que sÃ­, ya verÃ¡s.\n",
            "Predicted: ProbÃºe.\n",
            "------------------------------\n",
            "Sample #160\n",
            "Input:    Be fair.\n",
            "Actual:   SÃ© justo.\n",
            "Predicted: SÃ© buen.\n",
            "------------------------------\n",
            "Sample #170\n",
            "Input:    Be kind.\n",
            "Actual:   SÃ© amable.\n",
            "Predicted: Sean gentiles.\n",
            "------------------------------\n",
            "Sample #180\n",
            "Input:    Call me.\n",
            "Actual:   LlÃ¡mame.\n",
            "Predicted: Abrazosadmos.\n",
            "------------------------------\n",
            "Sample #190\n",
            "Input:    Fold it.\n",
            "Actual:   DÃ³blalo.\n",
            "Predicted: DespuÃ©sta.\n"
          ]
        }
      ],
      "source": [
        "# --- Test the model on 20 samples ---\n",
        "for seq_index in range(0, 200, 10): # Take 20 samples from the set\n",
        "    # Take one sequence\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "\n",
        "    # Run our function\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Sample #{seq_index}\")\n",
        "    print(f\"Input:    {input_texts[seq_index]}\")\n",
        "    # .strip() removes the \\t and \\n for clean viewing\n",
        "    print(f\"Actual:   {target_texts[seq_index].strip()}\")\n",
        "    print(f\"Predicted: {decoded_sentence.strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS5mQXPBl7Ul"
      },
      "source": [
        "-----\n",
        "\n",
        "### **Text Cell**\n",
        "\n",
        "## Cell 19: **New Cell** - Test Your Own Sentences\\!\n",
        "\n",
        "This is the best part. Why just test on old data? Let's write a function that lets you type a *new* English sentence and see the translation.\n",
        "\n",
        "This function will:\n",
        "\n",
        "1.  Take your string (e.g., \"How are you?\").\n",
        "2.  Convert it to the one-hot encoded `(1, max_seq_len, num_tokens)` array that the `encoder_model` expects.\n",
        "3.  Call our `decode_sequence` function.\n",
        "4.  Print the result\\!\n",
        "\n",
        "**Note:** The model will only know characters that were in its original vocabulary. If you use a character it hasn't seen (like `*` or `#`), it will fail.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Code Cell**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhH7cc7Pl7Ul",
        "outputId": "dad2f540-b036-4f94-873b-834f7ad0e4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translating: 'How are you?'\n",
            "Spanish:   Â¿CÃ³mo estÃ¡ tuenta?\n",
            "------------------------------\n",
            "Translating: 'I love you.'\n",
            "Spanish:   Te entiendo.\n",
            "------------------------------\n",
            "Translating: 'Stop!'\n",
            "Spanish:   Â¡Parad!\n",
            "------------------------------\n",
            "Translating: 'Run.'\n",
            "Spanish:   Prueba un poco.\n"
          ]
        }
      ],
      "source": [
        "def translate_sentence(input_text):\n",
        "    print(f\"Translating: '{input_text}'\")\n",
        "\n",
        "    # 1. Create the empty input array\n",
        "    input_seq = np.zeros(\n",
        "        (1, max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # 2. Fill it with one-hot vectors for the input text\n",
        "    for t, char in enumerate(input_text):\n",
        "        if char in input_token_index:\n",
        "            input_seq[0, t, input_token_index[char]] = 1.0\n",
        "        else:\n",
        "            print(f\"Warning: Unknown character '{char}' skipped.\")\n",
        "\n",
        "    # Pad the rest with spaces\n",
        "    input_seq[0, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "\n",
        "    # 3. Decode the sequence\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "    print(f\"Spanish:   {decoded_sentence.strip()}\")\n",
        "\n",
        "\n",
        "# --- Try it out! ---\n",
        "translate_sentence(\"How are you?\")\n",
        "print(\"-\" * 30)\n",
        "translate_sentence(\"I love you.\")\n",
        "print(\"-\" * 30)\n",
        "translate_sentence(\"Stop!\")\n",
        "print(\"-\" * 30)\n",
        "translate_sentence(\"Run.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
