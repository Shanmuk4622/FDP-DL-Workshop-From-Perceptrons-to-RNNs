{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Day 4: Transfer Learning (The Pro Move)\n",
        "Welcome to Day 4!\n",
        "\n",
        "On Day 3, we built a \"pro-level\" CNN that was stable, highly accurate (~98-99%), and didn't overfit. We are now very good at building and training a network from scratch.\n",
        "\n",
        "But what if we didn't have to?\n",
        "\n",
        "Today's Goal: We will learn Transfer Learning, the most powerful and common technique used in modern computer vision.\n",
        "\n",
        "Instead of building a \"baby\" network that has never seen an image, we will download a \"pro\" network (like VGG16) that has already been trained by experts at Google or Oxford on millions of images.\n",
        "\n",
        "Today's Plan:\n",
        "\n",
        "Load Data: Prepare our fruit dataset one last time.\n",
        "\n",
        "Theory: What is Transfer Learning?\n",
        "\n",
        "Load a Pre-trained Model (VGG16): We'll download the VGG16 model, pre-trained on the \"ImageNet\" dataset.\n",
        "\n",
        "\"Freeze\" the Base: We'll lock the original VGG16 layers so they don't change.\n",
        "\n",
        "Build Our \"Head\": We'll add our own Dense layers on top of VGG16 to solve our specific fruit problem.\n",
        "\n",
        "Train (Feature Extraction): We'll train only our new top layers.\n",
        "\n",
        "Theory: What is Fine-Tuning?\n",
        "\n",
        "\"Unfreeze\" and Fine-Tune: We'll unlock a few of the top VGG16 layers and re-train them with a tiny learning rate to get even better results.\n",
        "\n",
        "Compare: We'll compare our Day 2, Day 3, and Day 4 models to see the incredible power of this technique."
      ],
      "metadata": {
        "id": "sQLtPFYSk0Af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 1: Setup - Importing Libraries"
      ],
      "metadata": {
        "id": "gfOVoZR7k2pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import math\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(\"All libraries imported.\")"
      ],
      "metadata": {
        "id": "l_egw1o9k3y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 2: Define Dataset Parameters\n",
        "This time, we must change IMG_SIZE to 224."
      ],
      "metadata": {
        "id": "98a9OjGZk8NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will resize all images to 224x224\n",
        "IMG_SIZE = 224\n",
        "print(f\"Image size set to: {IMG_SIZE}x{IMG_SIZE} pixels\")"
      ],
      "metadata": {
        "id": "qGo178c0k-NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Learning Note: Why IMG_SIZE = 224?\n",
        "\n",
        "The VGG16 model we are about to use was pre-trained on the ImageNet dataset, where all images were 224x224 pixels.\n",
        "\n",
        "We must use the same image size that the model was originally trained on. If we gave it 64x64 images, the internal dimensions would not match, and the model would fail."
      ],
      "metadata": {
        "id": "h0v38phelAiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use a smaller batch size since the images are much larger\n",
        "BATCH_SIZE = 16\n",
        "print(f\"Batch size set to: {BATCH_SIZE}\")\n",
        "\n",
        "# Our images have 3 color channels (R, G, B)\n",
        "CHANNELS = 3\n",
        "print(f\"Number of color channels: {CHANNELS}\")\n",
        "\n",
        "# Our dataset has 6 classes\n",
        "NUM_CLASSES = 6\n",
        "print(f\"Number of classes: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "id": "7PrspYWPlD3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 3: Define File Paths"
      ],
      "metadata": {
        "id": "BZxE-i3BlFOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Ececute this cell only when you have Dataset already downloaded else , go to next cell\n",
        "# =============================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Base directory\n",
        "base_dir = r\"D:\\dataset\"  #set your dataset path\n",
        "print(f\"Base directory: {base_dir}\")\n",
        "print(\"Exists:\", os.path.exists(base_dir))\n",
        "\n",
        "# Training data path\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "print(f\"Training data path: {train_dir}\")\n",
        "print(\"Exists:\", os.path.exists(train_dir))\n",
        "\n",
        "# Test data path\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "print(f\"Test data path: {test_dir}\")\n",
        "print(\"Exists:\", os.path.exists(test_dir))\n"
      ],
      "metadata": {
        "id": "s2P8ozshlGaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# Execute this code only when you Dont have dataset || prefer when you are executing this in Google colab\n",
        "# ==============================================\n",
        "# Download the dataset from Kaggle using kagglehub\n",
        "\n",
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sriramr/fruits-fresh-and-rotten-for-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# The downloaded dataset structure is typically dataset/train and dataset/test\n",
        "# Adjust the base_dir to point to the extracted dataset folder\n",
        "base_dir = os.path.join(path, \"dataset\")\n",
        "\n",
        "print(f\"Base directory: {base_dir}\")\n",
        "print(\"Exists:\", os.path.exists(base_dir))\n",
        "\n",
        "\n",
        "# Training data path\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "print(f\"Training data path: {train_dir}\")\n",
        "print(\"Exists:\", os.path.exists(train_dir))\n",
        "\n",
        "# Test data path\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "print(f\"Test data path: {test_dir}\")\n",
        "print(\"Exists:\", os.path.exists(test_dir))"
      ],
      "metadata": {
        "id": "WJcd4zDcIDAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 4: Load and Prepare Datasets"
      ],
      "metadata": {
        "id": "MCLssKk_lInR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Training Data\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "print(\"Loaded Training Data.\")\n",
        "\n",
        "# Load Validation/Test Data\n",
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "print(\"Loaded Validation (Test) Data.\")\n",
        "\n",
        "# Get Class Names\n",
        "class_names = train_dataset.class_names\n",
        "print(f\"Class names: {class_names}\")\n",
        "\n",
        "# Configure for Performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# We add .repeat() to make sure the dataset loops for each epoch\n",
        "train_ds = train_dataset.cache().repeat().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = validation_dataset.cache().repeat().prefetch(buffer_size=AUTOTUNE)\n",
        "print(\"Applied .cache(), .repeat(), and .prefetch() to both datasets.\")"
      ],
      "metadata": {
        "id": "5dY23i4olKD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate steps_per_epoch for training\n",
        "# (We know these numbers from the output of the cell above)\n",
        "steps_per_epoch = math.ceil(10901 / BATCH_SIZE)\n",
        "\n",
        "# Calculate validation_steps for testing\n",
        "validation_steps = math.ceil(2698 / BATCH_SIZE)\n",
        "\n",
        "print(f\"Total training steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Total validation steps per epoch: {validation_steps}\")"
      ],
      "metadata": {
        "id": "WGpgcGyFlL1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 5: What is Transfer Learning? (Theory)\n",
        "Transfer Learning is the process of using a model that was pre-trained on a large, general dataset (like ImageNet, which has 1.2 million images of 1000 classes like \"dog,\" \"car,\" \"cat,\" etc.) and adapting it to solve a new, specific task (like our fruit problem).\n",
        "\n",
        "Why do this?\n",
        "\n",
        "The pre-trained model (like VGG16) has already spent thousands of hours of GPU time learning to \"see.\"\n",
        "\n",
        "Its first layers already know how to find edges and corners.\n",
        "\n",
        "Its middle layers know how to find textures and shapes.\n",
        "\n",
        "Its upper layers know how to find complex objects (like fur, wheels, or eyes).\n",
        "\n",
        "We don't need to re-teach this! We can just \"transfer\" all that knowledge.\n",
        "\n",
        "Our Two-Step Process:\n",
        "\n",
        "Feature Extraction (Training): We will \"chop off\" the original top layer of VGG16 (which predicted 1000 classes) and \"freeze\" the entire base. We then add our own small Dense layer \"head\" (which predicts 6 fruit classes) and train only our new head.\n",
        "\n",
        "Fine-Tuning (Optimizing): After our new head is trained, we will \"unfreeze\" a few of the top layers from the VGG16 base and continue training everything with a very low learning rate. This allows the VGG model to \"fine-tune\" its high-level feature detectors (e.g., \"fur\") to become better at detecting our specific features (e.g., \"rotten spots\")."
      ],
      "metadata": {
        "id": "W4a5iOXElOPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 6: Load the Pre-Trained VGG16 Base\n",
        "Let's load the VGG16 model from Keras. We'll pass three important arguments:\n",
        "\n",
        "weights='imagenet': This automatically downloads the weights it learned from the ImageNet dataset.\n",
        "\n",
        "include_top=False: This is the key. It means \"don't include the final 1000-neuron Dense layer.\" We're \"chopping off the head.\"\n",
        "\n",
        "input_shape=(...): We tell the model what size our images are."
      ],
      "metadata": {
        "id": "-waxP4BnlRmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Load the VGG16 model, pre-trained on ImageNet, without its top classifier\n",
        "base_model_vgg = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,  # This is the key: we want to build our *own* classifier head\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "\n",
        "print(\"VGG16 base model loaded.\")"
      ],
      "metadata": {
        "id": "yaxk51Szlulf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 7: Freeze the VGG16 Base\n",
        "This is a critical step. We need to tell Keras, \"Do NOT update the weights of the VGG16 layers during our training.\" We want to keep all the knowledge it already has."
      ],
      "metadata": {
        "id": "5Mcyd-nplxGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the base model's layers to be non-trainable\n",
        "base_model_vgg.trainable = False\n",
        "print(\"VGG16 base model is now 'frozen' (non-trainable).\")"
      ],
      "metadata": {
        "id": "TEK6ySc0lyh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the summary\n",
        "print(\"\\n--- VGG16 Summary ---\")\n",
        "base_model_vgg.summary()"
      ],
      "metadata": {
        "id": "8mEP1a3OlzoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analysis:\n",
        "\n",
        "Look at that! This base model has 14.7 Million parameters (all dedicated to finding features).\n",
        "\n",
        "And at the bottom, it shows: Trainable params: 0. This confirms our model is \"frozen.\""
      ],
      "metadata": {
        "id": "fUZfN8o6l1z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 8: Build Our New Model (Model 4)\n",
        "Now, we'll build our new model using the Functional API.\n",
        "\n",
        "Input: Our (224, 224, 3) image.\n",
        "\n",
        "Preprocessing: We need to use a special preprocess_input function that VGG16 expects. This normalizes the pixels in the exact same way the model was originally trained (e.g., scaling pixel values to -1 to 1 instead of 0 to 1).\n",
        "\n",
        "VGG16 Base: We'll pass the preprocessed image to our frozen base_model_vgg.\n",
        "\n",
        "Our \"Head\": We'll add a GlobalAveragePooling2D layer, a Dense layer, and our Dropout layer, just like our \"Pro\" model from Day 3.\n",
        "\n",
        "Output: Our final 6-class softmax layer."
      ],
      "metadata": {
        "id": "BtlOxeYkl4cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.layers import Lambda # <-- Make sure to import this\n",
        "\n",
        "# --- Build the Transfer Learning Model (Functional API) ---\n",
        "\n",
        "# 1. Input Layer\n",
        "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='input_image')\n",
        "\n",
        "# 2. Preprocessing Layer\n",
        "# We wrap the function in a Lambda layer to make it part of the model\n",
        "x = Lambda(preprocess_input, name='vgg_preprocessing')(inputs)\n",
        "\n",
        "# 3. VGG16 Base\n",
        "# We run the frozen base model in \"inference mode\" (training=False)\n",
        "# THE FIX: We remove name='vgg16_base' from this call.\n",
        "x = base_model_vgg(x, training=False)\n",
        "\n",
        "# 4. Our New \"Head\" (similar to Day 3)\n",
        "x = layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
        "x = layers.Dense(128, activation='relu', name='dense_head')(x)\n",
        "x = layers.Dropout(0.4, name='dropout_head')(x)\n",
        "\n",
        "# 5. Output Layer\n",
        "outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='output_layer')(x)\n",
        "\n",
        "# Create the final model\n",
        "model_vgg = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "print(\"Transfer Learning model with VGG16 base built successfully.\")"
      ],
      "metadata": {
        "id": "zPNtNrpUl6SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 9: Model 4 Summary\n",
        "Let's look at the summary for our new combined model."
      ],
      "metadata": {
        "id": "SDf6mckIl9Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg.summary()"
      ],
      "metadata": {
        "id": "yvWqY0bHl-w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analysis: The Best of Both Worlds!\n",
        "\n",
        "Look at the parameters at the bottom:\n",
        "\n",
        "Total params: 14,780,294 (14.7 Million!)\n",
        "\n",
        "Trainable params: 66,310 (Only 66 thousand!)\n",
        "\n",
        "Non-trainable params: 14,713,984\n",
        "\n",
        "This is perfect! The 14.7M \"frozen\" parameters from VGG16 will do all the heavy lifting of \"seeing,\" and we only have to train our tiny 66k parameter \"head\" to do the final classification."
      ],
      "metadata": {
        "id": "mSiCnB9wmAdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 10: Compile the Model 4"
      ],
      "metadata": {
        "id": "zzJwmfTnmCrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"VGG Transfer Learning model compiled.\")"
      ],
      "metadata": {
        "id": "23FYc2cWmD5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 11: Train the Model (Feature Extraction)\n",
        "Let's train our new model. We will only train for 5 epochs.\n",
        "\n",
        "Watch the val_accuracy. Because the VGG16 base is so powerful, we should see extremely high accuracy almost immediately."
      ],
      "metadata": {
        "id": "gtbAuA2fmF2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting VGG model training (Feature Extraction) for 5 epochs...\")\n",
        "# We save the history to plot it\n",
        "history_vgg = model_vgg.fit(\n",
        "    train_ds,\n",
        "    epochs=5,\n",
        "    validation_data=val_ds,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    verbose=1 # Show progress\n",
        ")\n",
        "\n",
        "print(\"\\nVGG training complete.\")"
      ],
      "metadata": {
        "id": "rUWsu16gmHqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 12: Visualize VGG Training History (Plots)"
      ],
      "metadata": {
        "id": "OYAd8niGmJNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the new history to a DataFrame\n",
        "history_vgg_df = pd.DataFrame(history_vgg.history)\n",
        "\n",
        "# Plot Loss\n",
        "history_vgg_df[['loss', 'val_loss']].plot(figsize=(10, 6))\n",
        "plt.title(\"VGG Transfer Learning: Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy\n",
        "history_vgg_df[['accuracy', 'val_accuracy']].plot(figsize=(10, 6))\n",
        "plt.title(\"VGG Transfer Learning: Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E961lSi6mK34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 13: Analyze the Results\n",
        "This is incredible!\n",
        "\n",
        "Look at your plots. The val_accuracy (orange) line probably started at ~95% or higher and may have ended around 98-99%... in only 5 epochs!\n",
        "\n",
        "This is the power of Transfer Learning. We didn't have to teach the model what an \"edge\" or \"texture\" is. We just taught our tiny 66k-parameter \"head\" how to connect VGG16's advanced features to our 6 fruit classes.\n",
        "\n",
        "###Now, let's try to get that last 1-2% with Fine-Tuning."
      ],
      "metadata": {
        "id": "CVWpOkuymPIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 14: Save the VGG Feature-Extraction Model"
      ],
      "metadata": {
        "id": "wZ4YpZxumSrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define file name ---\n",
        "vgg_model_path = \"day4_vgg_model.keras\"\n",
        "\n",
        "# --- Save the VGG Feature Extraction Model ---\n",
        "try:\n",
        "    print(f\"\\nSaving VGG model to: {vgg_model_path} ...\")\n",
        "    model_vgg.save(vgg_model_path)\n",
        "    print(\"VGG model saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving model_vgg: {e}\")"
      ],
      "metadata": {
        "id": "Lh0q5B4FmUFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 15: What is \"Fine-Tuning\"? (Theory)\n",
        "Fine-Tuning is the optional second step of transfer learning.\n",
        "\n",
        "First, we did Feature Extraction (Cells 11-13). We \"froze\" 100% of the VGG16 base and trained our new \"head.\"\n",
        "\n",
        "Now, we do Fine-Tuning. We will \"unfreeze\" the last few layers of the VGG16 base and continue training everything (the unfrozen base layers + our head) with a very, very low learning rate.\n",
        "\n",
        "Why do this?\n",
        "\n",
        "The last few layers of VGG16 are specialized for \"ImageNet\" (e.g., they know \"fur\" and \"feathers\"). By unfreezing them, we allow our optimizer to \"fine-tune\" them to be better at our task (e.g., \"rotten spots\" and \"banana curves\").\n",
        "\n",
        "###Why the low learning rate?\n",
        "\n",
        "We must use a very low learning rate (like 1e-5). If we use a high rate like adam's default (1e-3), the optimizer will make huge, destructive changes to the carefully pre-trained VGG weights, ruining all the knowledge. We just want to \"nudge\" them, not rewrite them."
      ],
      "metadata": {
        "id": "WYj_PTJpmWCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 16: Unfreeze the Top Layers\n",
        "First, let's \"unfreeze\" the VGG base so we can change its weights."
      ],
      "metadata": {
        "id": "2ulVSZUomZuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the base\n",
        "base_model_vgg.trainable = True\n",
        "\n",
        "# Let's see how many layers are in the base\n",
        "print(f\"There are {len(base_model_vgg.layers)} layers in the VGG16 base model.\")"
      ],
      "metadata": {
        "id": "e1itVk5AmbPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will freeze all layers EXCEPT the last 4\n",
        "# This means we will fine-tune 'block5_conv3' and 'block5_pool'\n",
        "fine_tune_at = -4\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model_vgg.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "print(\"Unfroze the top 4 layers of VGG16 for fine-tuning.\")"
      ],
      "metadata": {
        "id": "KKcocdP-mcVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 17: Compile for Fine-Tuning\n",
        "This is the most critical step of fine-tuning. We must re-compile the model, this time with a very low learning rate."
      ],
      "metadata": {
        "id": "PssKgFUNmejl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the Adam optimizer but with a much lower learning rate\n",
        "optimizer_fine_tune = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "\n",
        "model_vgg.compile(\n",
        "    optimizer=optimizer_fine_tune,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model re-compiled for fine-tuning with a low learning rate (1e-5).\")"
      ],
      "metadata": {
        "id": "HM0Ebxm1mgQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the summary again\n",
        "print(\"\\n--- Model Summary (for Fine-Tuning) ---\")\n",
        "model_vgg.summary()"
      ],
      "metadata": {
        "id": "Q0RF6-yymh4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analysis:\n",
        "\n",
        "Look at the parameters now!\n",
        "\n",
        "Total params: 14,780,294\n",
        "\n",
        "Trainable params: 7,149,062 (7.1 Million!)\n",
        "\n",
        "Non-trainable params: 7,631,232\n",
        "\n",
        "We are now re-training our 66k head PLUS over 7 million parameters from the top of the VGG16 model."
      ],
      "metadata": {
        "id": "03aoasXxmjzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 18: Train the Model (Fine-Tuning)\n",
        "Let's continue training our model for 10 more epochs. We will use the initial_epoch=5 argument to tell the model to \"start counting\" from epoch 5, which is where our first training run left off."
      ],
      "metadata": {
        "id": "IkV4uQCSmoHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We set initial_epoch to continue where we left off\n",
        "initial_epoch = 5\n",
        "fine_tune_epochs = 10\n",
        "total_epochs = initial_epoch + fine_tune_epochs\n",
        "\n",
        "print(f\"Starting VGG model fine-tuning for {fine_tune_epochs} more epochs...\")\n",
        "# We'll save the history again\n",
        "history_vgg_fine_tune = model_vgg.fit(\n",
        "    train_ds,\n",
        "    epochs=total_epochs,\n",
        "    initial_epoch=initial_epoch,  # Start counting from here\n",
        "    validation_data=val_ds,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nVGG fine-tuning complete.\")"
      ],
      "metadata": {
        "id": "KmP-T-XUmqjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 19: Visualize Fine-Tuning History\n",
        "Let's plot both training sessions on one graph to see the full story."
      ],
      "metadata": {
        "id": "G8if_qwVms05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the history from the first training run with the fine-tuning run\n",
        "\n",
        "# Get the first 5 epochs from the feature extraction run\n",
        "acc = history_vgg.history['accuracy']\n",
        "val_acc = history_vgg.history['val_accuracy']\n",
        "loss = history_vgg.history['loss']\n",
        "val_loss = history_vgg.history['val_loss']\n",
        "\n",
        "# Add the 10 epochs from the fine-tuning run\n",
        "acc.extend(history_vgg_fine_tune.history['accuracy'])\n",
        "val_acc.extend(history_vgg_fine_tune.history['val_accuracy'])\n",
        "loss.extend(history_vgg_fine_tune.history['loss'])\n",
        "val_loss.extend(history_vgg_fine_tune.history['val_loss'])\n",
        "\n",
        "# Plot the combined accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.axvline(initial_epoch - 1, color='red', linestyle='--', label='Start Fine-Tuning') # Add a line\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('VGG Training: Feature Extraction vs. Fine-Tuning')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot the combined loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.axvline(initial_epoch - 1, color='red', linestyle='--', label='Start Fine-Tuning') # Add a line\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('VGG Training: Feature Extraction vs. Fine-Tuning')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IMR6XSoVmu6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 20: Analyze the Results (Fine-Tuning)\n",
        "Look at the plots. You should see:\n",
        "\n",
        "Epochs 0-4 (Feature Extraction): A rapid jump in accuracy as our new \"head\" learns.\n",
        "\n",
        "Epoch 5 (The \"Start Fine-Tuning\" line): A small dip in accuracy. This is normal! The model is adjusting to the new, low learning rate and the \"unfrozen\" layers.\n",
        "\n",
        "Epochs 5-15 (Fine-Tuning): A slow, steady climb as the model \"fine-tunes\" the VGG layers, squeezing out that last bit of performance to get an even higher final accuracy.\n",
        "\n",
        "####We have successfully used Transfer Learning and Fine-Tuning!"
      ],
      "metadata": {
        "id": "DJjDsU2FmxXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 21: Save the Final Fine-Tuned Model"
      ],
      "metadata": {
        "id": "ST5rPBVZm1I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define file name ---\n",
        "final_vgg_model_path = \"day4_vgg_fine_tuned_model.keras\"\n",
        "\n",
        "# --- Save the Final VGG Model ---\n",
        "try:\n",
        "    print(f\"\\nSaving Final VGG model to: {final_vgg_model_path} ...\")\n",
        "    model_vgg.save(final_vgg_model_path)\n",
        "    print(\"Final VGG model saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving the final model: {e}\")"
      ],
      "metadata": {
        "id": "aCORJ3Jlnt7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 22: Final Predictions (VGG Model)\n",
        "Let's run our best model, the fine-tuned VGG16, on a batch of test images. We should expect near-perfect results."
      ],
      "metadata": {
        "id": "y7_mf68Env4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch of images and labels from the validation set\n",
        "images_batch, labels_batch = next(iter(val_ds))\n",
        "\n",
        "# Make predictions on this batch using the FINAL VGG model\n",
        "print(\"Making predictions with model_vgg on a batch of validation images...\")\n",
        "predictions_batch = model_vgg.predict(images_batch)\n",
        "\n",
        "# Get the predicted class indices\n",
        "predicted_indices = np.argmax(predictions_batch, axis=1)\n",
        "# Get the true class indices\n",
        "true_indices = np.argmax(labels_batch.numpy(), axis=1)\n",
        "\n",
        "# --- Plot the results ---\n",
        "plt.figure(figsize=(10, 10))\n",
        "print(\"Plotting Final VGG prediction grid...\")\n",
        "\n",
        "# Plot the first 9 images in the batch\n",
        "# Note: The images will look \"weird\" because they are still 0-255 floats\n",
        "# but matplotlib will auto-scale them for display.\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images_batch[i].numpy().astype(\"uint8\"))\n",
        "\n",
        "    pred_label = class_names[predicted_indices[i]]\n",
        "    true_label = class_names[true_indices[i]]\n",
        "\n",
        "    if pred_label == true_label:\n",
        "        color = 'green'\n",
        "    else:\n",
        "        color = 'red'\n",
        "\n",
        "    plt.title(f\"Pred: {pred_label}\\nTrue: {true_label}\", color=color)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iXDr4E1HnxKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 23: Peeking Inside the \"Mind\" of the VGG16 Model\n",
        "Let's do the same visualization we did for our simple CNN. We'll take one test image and see what it looks like as it passes through the powerful VGG16 model.\n",
        "\n",
        "We will:\n",
        "\n",
        "Grab a single test image (e.g., a 'rotten apple').\n",
        "\n",
        "Show the \"preprocessed\" image (what VGG16 actually sees).\n",
        "\n",
        "Show the feature maps from the pooling layers of each block to see how the image becomes more abstract.\n",
        "\n",
        "Show the final vector outputs from GlobalAveragePooling2D and the final prediction."
      ],
      "metadata": {
        "id": "ByyPfixXn0Q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cell: 23a set-up\n"
      ],
      "metadata": {
        "id": "PT-RKhcDoC40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# --- 1. Get a single image to test ---\n",
        "# Let's get one batch from the validation set\n",
        "images_batch, labels_batch = next(iter(val_ds))\n",
        "\n",
        "# Let's find a 'rottenapples' (index 3) to make it interesting\n",
        "try:\n",
        "    img_index = np.where(np.argmax(labels_batch.numpy(), axis=1) == 3)[0][0]\n",
        "    img_to_visualize = images_batch[img_index]\n",
        "    label_index = labels_batch.numpy()[img_index]\n",
        "\n",
        "    # The model expects a \"batch\", so we add one dimension\n",
        "    img_for_model = tf.expand_dims(img_to_visualize, axis=0)\n",
        "\n",
        "    print(f\"Loaded one image with shape: {img_for_model.shape}\")\n",
        "    print(f\"True Label: {class_names[np.argmax(label_index)]}\")\n",
        "\n",
        "except IndexError:\n",
        "    print(\"Could not find a 'rottenapples' in the first batch, using the first image instead.\")\n",
        "    img_to_visualize = images_batch[0]\n",
        "    label_index = labels_batch.numpy()[0]\n",
        "    img_for_model = tf.expand_dims(img_to_visualize, axis=0)\n",
        "\n",
        "\n",
        "# Helper function to plot the feature maps\n",
        "def plot_feature_maps(maps, title, n_features, grid_cols):\n",
        "    grid_rows = int(np.ceil(n_features / grid_cols))\n",
        "    plt.figure(figsize=(grid_cols * 2, grid_rows * 2))\n",
        "    plt.suptitle(title, fontsize=16, y=1.02)\n",
        "\n",
        "    # Squeeze the batch dimension\n",
        "    maps = np.squeeze(maps)\n",
        "\n",
        "    # Handle the case where maps are (height, width, channels)\n",
        "    if len(maps.shape) == 3:\n",
        "        # Plot only the first n_features\n",
        "        for i in range(min(n_features, maps.shape[2])):\n",
        "            plt.subplot(grid_rows, grid_cols, i + 1)\n",
        "            plt.imshow(maps[:, :, i], cmap='viridis') # 'viridis' shows intensity well\n",
        "            plt.title(f\"Filter {i+1}\")\n",
        "            plt.axis('off')\n",
        "    # Handle the case of a single map (like the rescaled image)\n",
        "    elif len(maps.shape) == 2:\n",
        "        plt.imshow(maps, cmap='viridis')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3aCdl7awoG0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1: Original vs. Preprocessed\n",
        "The vgg16.preprocess_input function (in our Lambda layer) does more than just scale pixels to 0-1. It also changes the color channel order and subtracts the ImageNet mean pixel value.\n",
        "\n",
        "This is why the \"preprocessed\" image looks so strange, but it's the exact format VGG16 was trained on."
      ],
      "metadata": {
        "id": "E3KOSzW0oLms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Get the Preprocessed Image ---\n",
        "# We create a small model to just get the output of the preprocessing layer\n",
        "try:\n",
        "    preprocessing_layer = model_vgg.get_layer('vgg_preprocessing')\n",
        "    viz_prep_model = Model(inputs=model_vgg.input, outputs=preprocessing_layer.output)\n",
        "    preprocessed_img = viz_prep_model.predict(img_for_model)\n",
        "\n",
        "    # --- 2. Plot them side-by-side ---\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(f\"Original from Batch: {img_to_visualize.shape}\")\n",
        "    plt.imshow(img_to_visualize.numpy().astype(\"uint8\"))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(f\"Preprocessed (VGG's view): {preprocessed_img.shape}\")\n",
        "    # We need to clip the values back to 0-255 for display\n",
        "    plt.imshow(np.clip(preprocessed_img[0], 0, 255).astype(\"uint8\"))\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during preprocessing visualization: {e}\")"
      ],
      "metadata": {
        "id": "k7uh-wN6oQPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2: Visualizing the Conv/Pool Layers\n",
        "Now, let's get the outputs from the pooling layer of each of the 5 VGG16 blocks.\n",
        "\n",
        "Notice how the image starts as (224, 224) and is shrunk down at each step, becoming more and more abstract. The final maps (block5_pool) are just 7x7 but contain all the high-level concepts the model has found."
      ],
      "metadata": {
        "id": "q-qOef_NoSzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Get the VGG Base Model ---\n",
        "# We need to get the layers from the 'base_model_vgg' that is *inside* our main model\n",
        "# (Note: your base model might be named 'vgg16' in its summary, use that if so)\n",
        "try:\n",
        "    base_model_layer = model_vgg.get_layer('vgg16') # This is the default name Keras gives it\n",
        "except ValueError:\n",
        "    # Fallback if you named the base model layer differently\n",
        "    base_model_layer = base_model_vgg\n",
        "\n",
        "# --- 2. List the layers we want to see ---\n",
        "layer_names = [\n",
        "    'block1_pool',\n",
        "    'block2_pool',\n",
        "    'block3_pool',\n",
        "    'block4_pool',\n",
        "    'block5_pool'\n",
        "]\n",
        "layer_outputs = [base_model_layer.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# --- 3. Create the visualization model ---\n",
        "viz_conv_model = Model(inputs=base_model_layer.input, outputs=layer_outputs)\n",
        "\n",
        "# --- 4. Get the activations ---\n",
        "# We must feed the *preprocessed_img* from the previous step\n",
        "conv_activations = viz_conv_model.predict(preprocessed_img)\n",
        "print(\"Conv/Pool activations generated.\")"
      ],
      "metadata": {
        "id": "L8VKoLzjoVH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the maps!\n",
        "# We'll just show the first 8 filters from each block to save space\n",
        "\n",
        "print(\"--- Block 1 Pool (112x112) ---\")\n",
        "plot_feature_maps(conv_activations[0], \"Block 1 Pool (112x112) - 8/64 Filters\", n_features=8, grid_cols=4)\n",
        "\n",
        "print(\"\\n--- Block 2 Pool (56x56) ---\")\n",
        "plot_feature_maps(conv_activations[1], \"Block 2 Pool (56x56) - 8/128 Filters\", n_features=8, grid_cols=4)\n",
        "\n",
        "print(\"\\n--- Block 3 Pool (28x28) ---\")\n",
        "plot_feature_maps(conv_activations[2], \"Block 3 Pool (28x28) - 8/256 Filters\", n_features=8, grid_cols=4)\n",
        "\n",
        "print(\"\\n--- Block 4 Pool (14x14) ---\")\n",
        "plot_feature_maps(conv_activations[3], \"Block 4 Pool (14x14) - 8/512 Filters\", n_features=8, grid_cols=4)\n",
        "\n",
        "print(\"\\n--- Block 5 Pool (7x7) ---\")\n",
        "plot_feature_maps(conv_activations[4], \"Block 5 Pool (7x7) - 8/512 Filters\", n_features=8, grid_cols=4)"
      ],
      "metadata": {
        "id": "tX_1ipWToYCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3: The Vector Part (From Features to Prediction)\n",
        "This is where the image stops being an image and becomes a 1D vector.\n",
        "\n",
        "GlobalAveragePooling2D: Takes the 512 7x7 feature maps from block5_pool and outputs a single vector of 512 numbers (the average of each map).\n",
        "\n",
        "Dense (Hidden): This 512-feature vector is fed to our hidden Dense layer (128 neurons).\n",
        "\n",
        "Dense (Output): The 128 neurons are fed to the final Dense layer (6 neurons), which gives us our final softmax prediction."
      ],
      "metadata": {
        "id": "OZ4w_u-qolZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. List the \"head\" layers ---\n",
        "layer_names = [\n",
        "    'global_avg_pool',\n",
        "    'dense_head',\n",
        "    'output_layer'\n",
        "]\n",
        "layer_outputs = [model_vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# --- 2. Create the \"head\" visualization model ---\n",
        "viz_head_model = Model(inputs=model_vgg.input, outputs=layer_outputs)\n",
        "\n",
        "# --- 3. Get the vector outputs ---\n",
        "# We feed the *original* image, as this model includes the preprocessor\n",
        "head_activations = viz_head_model.predict(img_for_model)\n",
        "\n",
        "# --- 4. Print the outputs ---\n",
        "print(\"--- Step 1: Global Average Pooling Output ---\")\n",
        "print(f\"Shape: {head_activations[0].shape}  (1 batch, 512 features)\")\n",
        "print(f\"First 10 values: {head_activations[0][0, :10]}\")\n",
        "\n",
        "print(\"\\n--- Step 2: Dense Hidden Layer (128 Neurons) Output ---\")\n",
        "print(f\"Shape: {head_activations[1].shape}\")\n",
        "print(f\"First 10 values: {head_activations[1][0, :10]}\")\n",
        "\n",
        "print(\"\\n--- Step 3: Dense Output Layer (6 Neurons) ---\")\n",
        "print(f\"Shape: {head_activations[2].shape}\")\n",
        "print(f\"Probabilities (softmax): \\n{head_activations[2][0]}\")\n",
        "\n",
        "# Find the final prediction\n",
        "final_prediction_index = np.argmax(head_activations[2][0])\n",
        "final_prediction_label = class_names[final_prediction_index]\n",
        "print(\"\\n---\")\n",
        "print(f\"FINAL PREDICTION: {final_prediction_label}\")"
      ],
      "metadata": {
        "id": "RnSAWJlvoqlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Day 4 Conclusion\n",
        "You have now seen the full spectrum of image classification!\n",
        "\n",
        "MLP (Failed): We proved that a simple Dense network is the wrong tool for image data. It overfits because it can't understand 2D space.\n",
        "\n",
        "Basic CNN (Good): We proved that Conv2D layers are the right tool. This model learned to see features and performed well (~97%).\n",
        "\n",
        "Pro CNN (Better): We made our CNN deeper and added BatchNormalization and GlobalAveragePooling2D. This reduced overfitting and created a very robust model (~98-99%).\n",
        "\n",
        "Transfer Learning (Best): We scrapped our own model and used VGG16, a model pre-trained on millions of images. We achieved the highest accuracy (~99%+) in the fewest epochs.\n",
        "\n",
        "###Takeaway: Never train from scratch if you don't have to! Transfer Learning is the fastest, most powerful way to get state-of-the-art results."
      ],
      "metadata": {
        "id": "fxImS6URoKXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it's Smart, Not Hard: Instead of training a model from scratch (like in Day 2 & 3), you've used Transfer Learning. You loaded VGG16, a massive model pre-trained by experts on millions of images (ImageNet). This means it already knows how to see edges, shapes, colors, and textures.\n",
        "\n",
        "Highly Efficient: Your notebook cleverly \"freezes\" the 14.7 million parameters of the VGG16 base and only trains a tiny new \"head\" (about 66,000 parameters) that you added. This is why it achieved very high accuracy in just 5 epochs.\n",
        "\n",
        "Pro-Level Technique: You then used Fine-Tuning by \"unfreezing\" the top layers of VGG16 and re-training them with a very low learning rate. This \"tunes\" the pre-trained features to be even better at your specific task of identifying fruit."
      ],
      "metadata": {
        "id": "PJKhpNbior9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have now seen the full spectrum of image classification!\n",
        "\n",
        "MLP (Failed): We proved that a simple Dense network is the wrong tool for image data.\n",
        "\n",
        "Basic CNN (Good): We proved that Conv2D layers are the right tool and got ~97% accuracy.\n",
        "\n",
        "Pro CNN (Better): We built a deeper, regularized model to get ~99% accuracy.\n",
        "\n",
        "Transfer Learning (Best): We \"borrowed the brain\" of VGG16, which already knew how to see. This gave us the highest accuracy (~99%+) in the fewest epochs."
      ],
      "metadata": {
        "id": "EGuLWDyeo_up"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zz0iIngTHxbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}