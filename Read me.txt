DAY-1 :
This Jupyter Notebook is a "Day 1" introduction to deep learning, guiding a user through building their first Artificial Neural Network (ANN) with TensorFlow and Keras. The goal is to classify handwritten digits from the classic MNIST dataset.

The notebook begins by importing necessary libraries (TensorFlow, Keras, Numpy, Matplotlib) and loading the MNIST dataset, which is split into 60,000 training images and 10,000 test images. It then inspects the data's shape ((28, 28) pixels) and visualizes the first image (a '5') to show the 0-255 pixel intensity range.


Before building a model, the data is preprocessed in two key steps:

Normalization: The pixel values are scaled from their 0-255 range down to a 0-1 range by dividing by 255.0. This helps the network train more stably.
Flattening: The 2D images (28x28) are reshaped into 1D vectors (784 elements) so they can be fed into a standard Dense layer.

As an optional baseline, the notebook first builds a "classical" machine learning model, a K-Nearest Neighbors (KNN) classifier, on a small subset of the data. This demonstrates that while KNN can achieve decent accuracy, it is very slow at making predictions.

Next, the first neural network (model_1) is built. It's a simple Sequential model containing just a Flatten layer (to handle the 2D image input) and a single Dense output layer with 10 neurons (one for each digit) using a softmax activation. This model is compiled with the adam optimizer and sparse_categorical_crossentropy loss function. After training for 5 epochs , it achieves an accuracy of about 92-93% on the test data.

To improve this result, a second, "deeper" network (model_2), known as a Multi-Layer Perceptron (MLP), is constructed. This model adds a Dense hidden layer of 128 neurons with the relu activation function between the input and output layers. This single addition dramatically improves performance, boosting the test accuracy to around 97-98%.

Finally, the notebook plots the training history (loss and accuracy) for both models to visually compare their performance. It then uses the superior model_2 to make predictions on the test set and displays a grid of 20 images, color-coding the predictions as green (correct) or red (incorrect). The tutorial concludes by summarizing the steps taken and introducing the topic for Day 2: Convolutional Neural Networks (CNNs).


DAY-2 :

Here is a summary of the two notebook files you've been working on and how they fit together.

These two notebooks are designed to be a single lesson (Day 2, Parts 1 and 2). They brilliantly demonstrate **why** standard neural networks (MLPs) are not good for image tasks and **how** Convolutional Neural Networks (CNNs) solve that problem.

### 1. `Day 2 - The Limits of MLPs part-1.ipynb` (The Problem)

This notebook sets up the challenge. Its main goal is to show the limitations of the MLP model we learned on Day 1.

* [cite_start]**New Challenge:** It introduces a more complex, real-world dataset: "Fresh vs. Rotten Fruits"[cite: 54]. These are 64x64 pixel color images.
* [cite_start]**New Skill:** It teaches an essential TensorFlow basic: how to load a custom dataset from folders using `image_dataset_from_directory`, which automatically labels the images based on the folder they're in (e.g., "freshapples", "rottenapples")[cite: 58, 59].
* [cite_start]**The "Gotcha" (The Failure):** The notebook has you build the same type of MLP model from Day 1. This requires a `Flatten` layer to unroll the `64x64x3` image into a single, massive `12,288`-feature vector [cite: 68-71].
* **The Result:** This `Flatten` step destroys all the spatial information (the 2D structure) of the image. [cite_start]The model becomes huge (over 1.5 million parameters!) and **overfits** badly[cite: 73, 74]. [cite_start]The training plots show the training accuracy shooting up while the *real* validation accuracy stays low (around 82%), proving the model is just memorizing, not learning [cite: 83-85].

### 2. `Day 2 - The Limits of MLPs part-2.ipynb` (The Solution)

This notebook picks up *exactly* where Part 1 failed and provides the solution.

* **The Theory:** It explains *why* the MLP failed (destroying spatial data) and introduces the two "magic" layers that make CNNs work:
    * **`Conv2D`:** A "filter" (like a magnifying glass) that slides over the 2D image to find simple features like edges and textures.
    * **`MaxPooling2D`:** A layer that shrinks the image to keep only the *strongest* features, making the model more efficient.
* **The "Magic" (The Solution):** It has you build a basic CNN. This model uses `Conv2D` and `MaxPooling2D` layers *first* to learn the features from the 2D image. Only *after* the features are learned is the data flattened and sent to a `Dense` layer.
* **The Result:** The CNN model is **~4 times smaller** (only ~407k parameters). More importantly, the training plots show the training and validation accuracy lines moving *together*. The final validation accuracy is much higher (around 96%+), proving the CNN is *actually learning* to generalize and identify fruit.

### The Main Lesson (MLP vs. CNN)

Together, these two files provide a perfect side-by-side comparison:

| Model | **Model 1: MLP (Part 1)** | **Model 2: CNN (Part 2)** |
| :--- | :--- | :--- |
| **How it "sees"** | [cite_start]`Flatten` layer destroys the 2D image[cite: 68]. | `Conv2D` layers preserve the 2D image. |
| **What it learns** | [cite_start]A jumbled list of 12,288 pixels[cite: 73]. | Spatial features (edges, curves, textures). |
| **Model Size** | [cite_start]**~1.58 Million** parameters[cite: 73]. | **~407 Thousand** parameters. |
| **Result** | [cite_start]Massive **Overfitting** [cite: 83-85]. | Successful **Learning** & **Generalization**. |
| **Accuracy** | ~82% | **~96%** |

The clear takeaway is that **MLPs (Dense layers) are the wrong tool for raw image data**, while **CNNs are the right tool** because they are designed to understand 2D spatial information.


